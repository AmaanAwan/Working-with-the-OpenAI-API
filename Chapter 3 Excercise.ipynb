{"cells":[{"source":"# Going Beyond Text Completions\n\nOpenAI has not only developed models for text generation but also for text moderation and audio transcription and translation. You'll learn to use OpenAI's moderation models to detect violations of their terms of use—a crucial function in user-facing applications. You'll also discover how the Whisper model can be used to transcribe and translate audio from different languages, which has huge applications in automated meeting notes and caption generation.","metadata":{},"cell_type":"markdown","id":"f7946cf6-b360-49ec-84b7-0a6d9ccd98f6"},{"source":"## Text moderation\n### Requesting moderation\nAside from text and chat completion models, OpenAI provides models with other capabilities, including text moderation. OpenAI's text moderation model is designed for evaluating prompts and responses to determine if they violate OpenAI's usage policies, including inciting hate speech and promoting violence.\n\nIn this exercise, you'll test out OpenAI's moderation functionality on a sentence that may have been flagged as containing violent content using traditional word detection algorithms.\n\n\n- Check if \"My favorite book is To Kill a Mockingbird.\" violates OpenAI’s policies using the Moderations endpoint.\n- Print the category scores to see the results.","metadata":{},"cell_type":"markdown","id":"ce3ce6ea-863e-48fd-bac0-f7b83396a156"},{"source":"client = OpenAI(api_key=\"<OPENAI_API_TOKEN>\")\n\n# Create a request to the Moderation endpoint\nresponse = client.moderations.create(\n    model=\"text-moderation-latest\",\n    input=\"My favorite book is To Kill a Mockingbird.\"\n)\n\n# Print the category scores\nprint(response.results[0].category_scores)","metadata":{},"cell_type":"code","id":"45e35c4e-a262-4188-b264-0d2feab5552f","outputs":[],"execution_count":null},{"source":"### Examining moderation category scores\nThe same request you created in the last exercise to the Moderation endpoint has been run again, sending the sentence \"My favorite book is To Kill a Mockingbird.\" to the model. The response from the API has been printed for you, and is available as response.\n\nWhat is the correct interpretation of the category_scores here?\n\n\n`ModerationCreateResponse(id='modr-BBRkPefIkB3xbchgGTDbJ96xDzy43', model='text-moderation-007', results=[Moderation(categories=Categories(harassment=False, harassment_threatening=False, hate=False, hate_threatening=False, illicit=None, illicit_violent=None, self_harm=False, self_harm_instructions=False, self_harm_intent=False, sexual=False, sexual_minors=False, violence=False, violence_graphic=False, self-harm=False, sexual/minors=False, hate/threatening=False, violence/graphic=False, self-harm/intent=False, self-harm/instructions=False, harassment/threatening=False), category_applied_input_types=None, category_scores=CategoryScores(harassment=5.243551186140394e-06, harassment_threatening=1.1516095810293336e-06, hate=4.767837526742369e-05, hate_threatening=3.2021056028952444e-08, illicit=None, illicit_violent=None, self_harm=9.466615438213921e-07, self_harm_instructions=5.426785065765216e-08, self_harm_intent=1.5536235764557205e-07, sexual=3.545879735611379e-06, sexual_minors=1.1304399549771915e-06, violence=0.0001064608441083692, violence_graphic=1.086988686438417e-05, self-harm=9.466615438213921e-07, sexual/minors=1.1304399549771915e-06, hate/threatening=3.2021056028952444e-08, violence/graphic=1.086988686438417e-05, self-harm/intent=1.5536235764557205e-07, self-harm/instructions=5.426785065765216e-08, harassment/threatening=1.1516095810293336e-06), flagged=False)])\n`\n\nPossible answers\n1. The model believes that the sentence contains violent content, as the violence category is close to 0.\n2. The model believes that there are no violations, as all categories are close to 0.\n3. The model believes that the sentence contains hate speech, as the hate category is close to 0.","metadata":{},"cell_type":"markdown","id":"5b4c4f73-1bef-4b70-a1c8-eaa47271311b"},{"source":"## Speech-to-Text Transcription with Whisper","metadata":{},"cell_type":"markdown","id":"e31755ad-f1f8-4bdc-b093-be8a2482a179"},{"source":"### Creating a podcast transcript\nThe OpenAI API Audio endpoint provides access to the Whisper model, which can be used for speech-to-text transcription and translation. In this exercise, you'll create a transcript from a DataFramed podcast episode with OpenAI Developer, Logan Kilpatrick.\n\nIf you'd like to hear more from Logan, check out the full ChatGPT and the OpenAI Developer Ecosystem podcast episode.\n\n\n- Open the openai-audio.mp3 file.\n- Create a transcription request to the Audio endpoint.\n- Extract and print the transcript text from the response.","metadata":{},"cell_type":"markdown","id":"33c4c8a2-2427-43f2-b19c-4dcaaf1443a7"},{"source":"client = OpenAI(api_key=\"<OPENAI_API_TOKEN>\")\n\n# Open the openai-audio.mp3 file\naudio_file = open(\"openai-audio.mp3\", \"rb\")\n\n# Create a transcript from the audio file\nresponse = client.audio.transcriptions.create(model=\"whisper-1\", file=audio_file)\n\n# Extract and print the transcript text\nprint(response.text)","metadata":{},"cell_type":"code","id":"4a80f176-6bf7-46f8-997d-963942ffea69","outputs":[],"execution_count":null},{"source":"### Transcribing a non-English language\nThe Whisper model can not only transcribe English language, but also performs well on speech in many other languages.\nIn this exercise, you’ll create a transcript from audio.m4a, which contains speech in Portuguese.\n \n- Open the audio.m4a file.\n- Create a request to the Audio endpoint to transcribe audio.m4a.","metadata":{},"cell_type":"markdown","id":"19c0efbf-8078-4376-a00d-7701da56f4f1"},{"source":"client = OpenAI(api_key=\"<OPENAI_API_TOKEN>\")\n\n# Open the audio.m4a file\naudio_file = open(\"audio.m4a\", \"rb\")\n\n# Create a transcript from the audio file\nresponse = client.audio.transcriptions.create(model=\"whisper-1\", file=audio_file)\n\nprint(response.text)","metadata":{},"cell_type":"code","id":"46473684-cd58-4e89-9349-9905e3e9feee","outputs":[],"execution_count":null},{"source":"## Speech Translation with Whisper","metadata":{},"cell_type":"markdown","id":"41674d69-1cf9-45d2-9953-86c3dceb2c8a"},{"source":"### Translating Portuguese\nWhisper can not only transcribe audio into its native language but also supports translation capabilities for creating English transcriptions.\n\nIn this exercise, you'll return to the Portuguese audio, but this time, you'll translate it into English!\n\n\n- Open the audio.m4a file.\n- Create a translation request to the Audio endpoint.\n- Extract and print the translated text from the response.","metadata":{},"cell_type":"markdown","id":"3ad05dcf-34f3-4f0b-9dec-514ac0412a15"},{"source":"client = OpenAI(api_key=\"<OPENAI_API_TOKEN>\")\n\n# Open the audio.m4a file\naudio_file = open(\"audio.m4a\", \"rb\")\n\n# Create a translation from the audio file\nresponse = client.audio.translations.create(model=\"whisper-1\", file=audio_file)\n\n# Extract and print the translated text\nprint(response.text)","metadata":{},"cell_type":"code","id":"ca36c48b-3d7d-4c19-9f35-efb9fd4b0b88","outputs":[],"execution_count":null},{"source":"### Translating with prompts\nThe quality of Whisper's translation can vary depending on the language spoken, the audio quality, and the model's awareness of the subject matter. If you have any extra context about what is being spoken about, you can send it along with the audio to the model to give it a helping hand.\n\nYou've been provided with an audio file, audio.wav; you're not sure what language is spoken in it, but you do know it relates to a recent World Bank report. Because you don't know how well the model will perform on this unknown language, you opt to send the model this extra context to steer it in the right direction.\n\n- Open the audio.wav file.\n- Write a prompt that informs the model that the audio relates to a recent World Bank report, which will help the model produce an accurate translation.\n- Create a request to the Audio endpoint to transcribe audio.wav using your prompt.","metadata":{},"cell_type":"markdown","id":"1fd0fdde-48e1-4ba9-854c-82b38363f4c6"},{"source":"client = OpenAI(api_key=\"<OPENAI_API_TOKEN>\")\n\n# Open the audio.wav file\naudio_file = open(\"audio.wav\", \"rb\")\n\n# Write an appropriate prompt to help the model\nprompt = \"the audio is related to world bank report.\"\n\n# Create a translation from the audio file\nresponse = client.audio.translations.create(model=\"whisper-1\",\n                                            file=audio_file,\n                                            prompt= prompt)\n\nprint(response.text)","metadata":{},"cell_type":"code","id":"87ad4ba8-6cf2-461f-9477-e1eb937b1cb8","outputs":[],"execution_count":null},{"source":"## Combining models","metadata":{},"cell_type":"markdown","id":"f7756468-539d-43a5-a089-8d5db42a2e42"},{"source":"### Identifying audio language\nYou've learned that you're not only limited to creating a single request, and that you can actually feed the output of one model as an input to another! This is called chaining, and it opens to the doors to more complex, multi-modal use cases.\n\nIn this exercise, you'll practice model chaining to identify the language used in an audio file. You'll do this by bringing together OpenAI's audio transcription functionality and its text models with only a few lines of code.\n\n- Open the audio.wav file and assign to audio_file.\n- Create a transcript from audio_file and assign to audio_response.\n- Prompt a text model using the text from audio_response to discover the language used in audio.wav.","metadata":{},"cell_type":"markdown","id":"5d9f35a1-a541-47fe-a519-b7c46630a0a4"},{"source":"client = OpenAI(api_key=\"<OPENAI_API_TOKEN>\")\n\n# Open the audio.wav file\naudio_file = open(\"audio.wav\", \"rb\")\n\n# Create a transcription request using audio_file\naudio_response = client.audio.transcriptions.create(model=\"whisper-1\", file=audio_file)\ntranscript=audio_response.text\n\n# Create a request to the API to identify the language spoken\nchat_response = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=[{\"role\":\"user\", \"content\":\"what is the language in the text\" + transcript}]\n)\nprint(chat_response.choices[0].message.content)","metadata":{},"cell_type":"code","id":"969d052c-14cd-4d70-b44a-611f576270b7","outputs":[],"execution_count":null},{"source":"### Creating meeting summaries\nTime for business! One time-consuming task that many find themselves doing day-to-day is taking meeting notes to summarize attendees, discussion points, next steps, etc.\n\nIn this exercise, you'll use AI to augment this task to not only save a substantial amount of time, but also to empower attendees to focus on the discussion rather than administrative tasks. You've been provided with a recording from DataCamp's Q2 Roadmap webinar, which summarizes what DataCamp will be releasing during that quarter. You'll chain the Whisper model with a text or chat model to discover which courses will be launched in Q2.\n\n- Open the datacamp-q2-roadmap.mp3 file and assign to audio_file.\n- Create a transcript from audio_file and assign to audio_response.\n- Prompt a text model using the text from transcript and summarize it into concise bullet points.","metadata":{},"cell_type":"markdown","id":"0e16c8cc-bb6c-464a-9894-47dc88caa3c6"},{"source":"client = OpenAI(api_key=\"<OPENAI_API_TOKEN>\")\n\n# Open the datacamp-q2-roadmap.mp3 file\naudio_file = open(\"datacamp-q2-roadmap.mp3\", \"rb\")\n\n# Create a transcription request using audio_file\naudio_response = client.audio.transcriptions.create(model=\"whisper-1\", file=audio_file)\ntranscript=audio_response.text\n\n# Create a request to the API to summarize the transcript into bullet points\nchat_response = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=[\n        {\"role\":\"user\", \"content\":\"summarize and give bullet points\"+ transcript}\n    ],\n    max_tokens=100\n)\nprint(chat_response.choices[0].message.content)","metadata":{},"cell_type":"code","id":"ad00678d-2857-49df-824c-879fee23925e","outputs":[],"execution_count":null}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":5}